{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.sparse.linalg import eigs\n",
    "from numpy.linalg import matrix_power\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse.linalg import eigs\n",
    "from numpy.linalg import matrix_power\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(x,y):\n",
    "    return np.mean(x!=y)\n",
    "\n",
    "def weighted_jaccard(x,y,w=None):\n",
    "    \"\"\"w is list of weights, same length as x and y summing to 1\"\"\"\n",
    "    if w is None:\n",
    "        return jaccard(x,y)\n",
    "    return (x!=y).dot(w)\n",
    "\n",
    "def jaccard_distance_matrix(X, n_jobs=1):\n",
    "    vint = np.vectorize(int)\n",
    "    X_int = vint(X*100)\n",
    "    return pairwise_distances(X_int, metric=jaccard, n_jobs=n_jobs)\n",
    "\n",
    "def weighted_jaccard_distance_matrix(X, w, n_jobs=1):\n",
    "    \"\"\"w has length X.shape[1]\"\"\"\n",
    "    vint = np.vectorize(int)\n",
    "    X_int = vint(X*100)\n",
    "    distance_matrix = pairwise_distances(X_int, w=w, metric=weighted_jaccard,n_jobs=n_jobs)\n",
    "    return distance_matrix\n",
    "\n",
    "class JKMedoids(object):\n",
    "    def __init__(self, k, max_iter=100, n_attempts=10, accepting_weights=True, weight_adjustment=0, n_jobs=1):\n",
    "        self.k = k\n",
    "        self.n_attempts = n_attempts\n",
    "        self.n_jobs=n_jobs\n",
    "        if max_iter is None:\n",
    "            self.max_iter = -1\n",
    "        else:\n",
    "            self.max_iter = max_iter\n",
    "        self.accepting_weights = accepting_weights\n",
    "        self.weight_adjustment = weight_adjustment\n",
    "        self.adjusting_weights = self.weight_adjustment != 0\n",
    "        self.distance_matrix = None\n",
    "        self.assignments = None\n",
    "        self.assignment_score = None\n",
    "        self.weights = None\n",
    "\n",
    "    def fit_once(self, X):\n",
    "        assignments = np.random.randint(0,self.k,size=X.shape[0])\n",
    "        old_assignments = np.zeros(assignments.shape)\n",
    "        it = 0\n",
    "        while (old_assignments != assignments).any() and it != self.max_iter:\n",
    "            it += 1\n",
    "            old_assignments = assignments\n",
    "\n",
    "            centroids = []\n",
    "            for cluster in range(self.k):\n",
    "                mask = assignments == cluster\n",
    "                if np.sum(mask) == 0:\n",
    "                    continue\n",
    "                within_cluster_distance_matrix = (self.distance_matrix[mask]).T\n",
    "                most_central_point = np.argmin(np.sum(within_cluster_distance_matrix,1))\n",
    "                centroids.append(most_central_point)\n",
    "\n",
    "            to_centroid_distance_matrix = (self.distance_matrix[centroids]).T\n",
    "            assignments = np.apply_along_axis(np.argmin, 1, to_centroid_distance_matrix)\n",
    "\n",
    "            if self.adjusting_weights:\n",
    "                weight_update = np.apply_along_axis(lambda col: mutual_info_score(assignments, col), 0, X)\n",
    "                weight_update = weight_update/np.sum(weight_update)\n",
    "                self.weights = self.weights*(1-self.weight_adjustment) + weight_update*self.weight_adjustment\n",
    "                self.distance_matrix = weighted_jaccard_distance_matrix(X, self.weights)\n",
    "        return assignments\n",
    "\n",
    "    def score(self, assignments):\n",
    "        centroids = []\n",
    "        for cluster in range(self.k):\n",
    "            mask = assignments == cluster\n",
    "            if np.sum(mask) == 0:\n",
    "                continue\n",
    "            within_cluster_distance_matrix = (self.distance_matrix[mask]).T\n",
    "            most_central_point = np.argmin(np.sum(within_cluster_distance_matrix,1))\n",
    "            centroids.append(most_central_point)\n",
    "        to_centroid_distance_matrix = (self.distance_matrix[centroids]).T\n",
    "        scores = np.apply_along_axis(np.min, 1, to_centroid_distance_matrix)\n",
    "        score = np.sum(scores)\n",
    "        return score\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self.accepting_weights:\n",
    "            X, self.weights = X\n",
    "            self.distance_matrix = weighted_jaccard_distance_matrix(X, self.weights, self.n_jobs)\n",
    "        else:\n",
    "            self.distance_matrix = jaccard_distance_matrix(X, self.n_jobs)\n",
    "        for _ in range(self.n_attempts):\n",
    "            assignments = self.fit_once(X)\n",
    "            if self.assignments is None:\n",
    "                self.assignments = assignments\n",
    "                self.assignment_score = self.score(self.assignments)\n",
    "            else:\n",
    "                score = self.score(assignments)\n",
    "                if score < self.assignment_score:\n",
    "                    self.assignment_score = score\n",
    "                    self.assignments = assignments\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X, _=None):\n",
    "        self.fit(X)\n",
    "        return self.assignments\n",
    "\n",
    "class SquishyJKMedoids(object):\n",
    "    def __init__(self, k, max_iter=100, n_attempts=10, accepting_weights=True, weight_adjustment=0, n_jobs=1):\n",
    "        self.k = k\n",
    "        self.n_attempts = n_attempts\n",
    "        if max_iter is None:\n",
    "            self.max_iter = -1\n",
    "        else:\n",
    "            self.max_iter = max_iter\n",
    "        self.accepting_weights = accepting_weights\n",
    "        self.distance_matrix = None\n",
    "        self.to_centroid_distances = None\n",
    "        self.assignments = None\n",
    "        self.assignment_score = None\n",
    "        self.n_jobs=n_jobs\n",
    "\n",
    "    def fit_once(self, X):\n",
    "        assignments = np.random.randint(0,self.k,size=X.shape[0])\n",
    "        old_assignments = np.zeros(assignments.shape)\n",
    "        it = 0\n",
    "        while (old_assignments != assignments).any() and it != self.max_iter:\n",
    "            it += 1\n",
    "            old_assignments = assignments\n",
    "\n",
    "            self.to_centroid_distances = []\n",
    "            centroids = []\n",
    "            first_run = True\n",
    "            for cluster in range(self.k):\n",
    "                mask = assignments == cluster\n",
    "                if np.sum(mask) == 0:\n",
    "                    continue\n",
    "                if first_run:\n",
    "                    distance_matrix = self.distance_matrix\n",
    "                else:\n",
    "                    weights = np.apply_along_axis(lambda col: mutual_info_score(mask, col), 0, X)\n",
    "                    weights = weights/np.sum(weights)\n",
    "                    distance_matrix = weighted_jaccard_distance_matrix(X, weights, n_jobs=self.n_jobs)\n",
    "                within_cluster_distance_matrix = (distance_matrix[mask]).T\n",
    "                most_central_point = np.argmin(np.sum(within_cluster_distance_matrix,1))\n",
    "                centroids.append(most_central_point)\n",
    "                self.to_centroid_distances.append(distance_matrix[:,most_central_point].reshape(-1))\n",
    "\n",
    "            to_centroid_distance_matrix = (np.array(self.to_centroid_distances)).T\n",
    "            assignments = np.apply_along_axis(np.argmin, 1, to_centroid_distance_matrix)\n",
    "\n",
    "            first_run = False\n",
    "        return assignments\n",
    "\n",
    "    def score(self, assignments):\n",
    "        centroids = []\n",
    "        for cluster in range(self.k):\n",
    "            mask = assignments == cluster\n",
    "            if np.sum(mask) == 0:\n",
    "                continue\n",
    "            within_cluster_distance_matrix = (self.distance_matrix[mask]).T\n",
    "            most_central_point = np.argmin(np.sum(within_cluster_distance_matrix,1))\n",
    "            centroids.append(most_central_point)\n",
    "        to_centroid_distance_matrix = (self.distance_matrix[centroids]).T\n",
    "        scores = np.apply_along_axis(np.min, 1, to_centroid_distance_matrix)\n",
    "        score = np.sum(scores)\n",
    "        return score\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self.accepting_weights:\n",
    "            X, self.weights = X\n",
    "        else:\n",
    "            self.weights = np.ones(X.shape[1])/X.shape[1]\n",
    "        self.distance_matrix = weighted_jaccard_distance_matrix(X, self.weights, n_jobs=self.n_jobs)\n",
    "        for _ in range(self.n_attempts):\n",
    "            assignments = self.fit_once(X)\n",
    "            if self.assignments is None:\n",
    "                self.assignments = assignments\n",
    "                self.assignment_score = self.score(self.assignments)\n",
    "            else:\n",
    "                score = self.score(assignments)\n",
    "                if score < self.assignment_score:\n",
    "                    self.assignment_score = score\n",
    "                    self.assignments = assignments\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X, _=None):\n",
    "        self.fit(X)\n",
    "        return self.assignments\n",
    "\n",
    "\n",
    "\n",
    "class EigenvectorWeighting(object):\n",
    "    def __init__(self, complete=False, extent=1):\n",
    "        self.data = None\n",
    "        self.weights = None\n",
    "        self.complete = complete\n",
    "        self.extent = extent\n",
    "\n",
    "    def fit(self, data):\n",
    "        data, weights = data\n",
    "        self.data = data\n",
    "        mutual_info_matrix = pairwise_distances(data.T, metric=mutual_info_score)\n",
    "        if self.complete:\n",
    "            evals, evecs = eigs(mutual_info_matrix, k=1)\n",
    "            weights = evecs.reshape(-1)\n",
    "            self.weights = weights/np.sum(weights)\n",
    "        else:\n",
    "            weights = weights.reshape(-1,1)\n",
    "            mutual_info_power = matrix_power(mutual_info_matrix, self.extent)\n",
    "            weights = mutual_info_power.dot(weights)\n",
    "            weights = weights.reshape(-1)\n",
    "            self.weights = weights/np.sum(weights)\n",
    "\n",
    "    def transform(self, data, _=None):\n",
    "        self.fit(data)\n",
    "        return self.data, self.weights\n",
    "\n",
    "    def fit_transform(self, data, _=None):\n",
    "        self.fit(data)\n",
    "        return self.data, self.weights\n",
    "    \n",
    "\n",
    "### Helper functions to be parallelized, if possible\n",
    "def fit_rf_model(rftransform, X, i, features_to_predict):\n",
    "    y_temp = X[:, features_to_predict]\n",
    "    if rftransform.model_type == 'gradient_boosting':\n",
    "        y_temp = np.apply_along_axis(np.mean,1,y_temp)\n",
    "    X_temp = np.delete(X, features_to_predict, axis=1)\n",
    "    rf_fit = rftransform.rfs[i].fit(X_temp, y_temp)\n",
    "    return rf_fit\n",
    "\n",
    "def get_predictions(rftransform, X, i, features_to_predict):\n",
    "    y_temp = X[:, features_to_predict]\n",
    "    X_temp = np.delete(X, features_to_predict, axis=1)\n",
    "    predictions = rftransform.rfs[i].predict(X_temp)\n",
    "    if len(predictions.shape) > 1:\n",
    "        predictions = np.sum(predictions, 1)\n",
    "    return predictions\n",
    "\n",
    "def get_weight(rftransform, X, features_to_predict):\n",
    "    y_temp = X[:, features_to_predict]\n",
    "    y_temp_var = np.sum(np.apply_along_axis(np.var, 0, y_temp))\n",
    "    weight = (1/y_temp_var)**rftransform.weight_extent\n",
    "    return weight\n",
    "\n",
    "class RFTransform(object):\n",
    "    def __init__(self,\n",
    "                 n_forests,\n",
    "                 model_type='random_forest',\n",
    "                 n_trees=1,\n",
    "                 n_features_to_predict=0.5,\n",
    "                 max_depth=5,\n",
    "                 outputting_weights=True,\n",
    "                 using_pca=True,\n",
    "                 weight_extent=1,\n",
    "                 learning_rate=0.9,\n",
    "                 n_jobs=1):\n",
    "        self.n_forests = n_forests\n",
    "        self.n_trees = n_trees\n",
    "        self.n_features_to_predict = n_features_to_predict\n",
    "        self.outputting_weights = outputting_weights\n",
    "        self.weight_extent = weight_extent\n",
    "        self.n_jobs = n_jobs\n",
    "        self.model_type = model_type\n",
    "        if self.model_type == 'random_forest':\n",
    "            self.rfs = [RandomForestRegressor(n_trees, max_depth=max_depth, n_jobs=-1) for _ in range(n_forests)]\n",
    "        elif self.model_type == 'gradient_boosting':\n",
    "            self.rfs = [GradientBoostingRegressor(n_estimators=n_trees, learning_rate=learning_rate, max_depth=max_depth) for _ in range(n_forests)]\n",
    "        else:\n",
    "            raise ValueError(\"RFTransform.model_type must be 'random_forest' or 'gradient_boosting'.\")\n",
    "        self.using_pca = using_pca\n",
    "        if not self.using_pca and self.outputting_weights:\n",
    "            print ('warning')\n",
    "        self.pca = PCA()\n",
    "        self.ss1 = StandardScaler()\n",
    "        self.decision_paths = None\n",
    "        self.features_indices = []\n",
    "        if outputting_weights:\n",
    "            self.weights = []\n",
    "\n",
    "    def fit(self, X_init, *args, **kwargs):\n",
    "        self.features_indices = []\n",
    "\n",
    "        X_ss = self.ss1.fit_transform(X_init)\n",
    "        if self.using_pca:\n",
    "            X = self.pca.fit_transform(X_ss)\n",
    "        else:\n",
    "            X = X_ss\n",
    "        if isinstance(self.n_features_to_predict, float):\n",
    "            n_output = int(self.n_features_to_predict * X.shape[1])\n",
    "        elif isinstance(self.n_features_to_predict, int):\n",
    "            n_output = self.n_features_to_predict\n",
    "        elif self.n_features_to_predict == 'sqrt':\n",
    "            n_output = int(np.sqrt(X.shape[1]))\n",
    "        elif self.n_features_to_predict == 'log':\n",
    "            n_output = int(np.log2(X.shape[1]))\n",
    "\n",
    "        if n_output == 0:\n",
    "            n_output = 1\n",
    "\n",
    "        for i in range(self.n_forests):\n",
    "            features_to_predict = np.random.choice(np.arange(X.shape[1]),(n_output,),replace=False)\n",
    "            self.features_indices.append(features_to_predict)\n",
    "\n",
    "        self.rfs = Parallel(n_jobs=self.n_jobs)(delayed(fit_rf_model)(self, X, i, features_to_predict) for i, features_to_predict in enumerate(self.features_indices))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_init):\n",
    "        self.decision_paths = None\n",
    "        if self.outputting_weights:\n",
    "            self.weights = []\n",
    "\n",
    "        X_ss = self.ss1.transform(X_init)\n",
    "        if self.using_pca:\n",
    "            X = self.pca.transform(X_ss)\n",
    "        else:\n",
    "            X = X_ss\n",
    "\n",
    "        decision_paths = Parallel(n_jobs=self.n_jobs)(delayed(get_predictions)(self,X,i,features_to_predict)\n",
    "                                                      for i, features_to_predict in enumerate(self.features_indices))\n",
    "        self.decision_paths = (np.array(decision_paths)).T\n",
    "\n",
    "        if self.outputting_weights:\n",
    "            self.weights = Parallel(n_jobs=self.n_jobs)(delayed(get_weight)(self,X,features_to_predict) for features_to_predict in self.features_indices)\n",
    "            self.weights = np.array(self.weights)\n",
    "            self.weights = self.weights/np.sum(self.weights)\n",
    "            return self.decision_paths, self.weights\n",
    "        else:\n",
    "            return self.decision_paths\n",
    "\n",
    "    def fit_transform(self, X_init, *args, **kwargs):\n",
    "        self.fit(X_init)\n",
    "        return self.transform(X_init)\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "class RFCluster(Pipeline):\n",
    "    \"\"\"\n",
    "    A clustering algorithm wherein a forest of shallow trees is trained on random subsets of the features (each tree trained\n",
    "    on a different random subset). Points are clustered together according to how often they end up on the same leaf.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "      k: Int. Number of clusters to partition the data into. (Note: for large k, RFCluster often returns fewer than k clusters.\n",
    "            This occurs in the final k-medoids step. If, at any given step, one data point is selected as the next centroid for\n",
    "            two clusters, this reduces the number of clusters returned.\n",
    "      \n",
    "      n_trees: Int. Number of decision trees in the ensemble.\n",
    "      \n",
    "      n_features_to_predict: Float, int, or string. Determines how many features form the target for each decision tree.\n",
    "            If float, takes that fraction of the total number of features (i.e., if there are 20 features in\n",
    "            the data, and n_features_to_predict = 0.4, each decision tree will be trained to predict the\n",
    "            values of 8 features, given the remaining 12.\n",
    "            If int, that many features will be predicted with each decision tree.\n",
    "            If 'sqrt', n_features_to_predict will take on the square root of the total number of features.\n",
    "            If 'log', same as for 'sqrt', but using the log base 2 instead.\n",
    "      \n",
    "      max_depth: Int. The maximum depth of each decision tree.\n",
    "      \n",
    "      using_weights: Boolean. If True, when ensembling the partitions from each decision tree, some trees will have extra\n",
    "            weight, given the variance of the features that the decision trees are trained on.\n",
    "      \n",
    "      weight_extent: Nonnegative float. Only relevant if using_weights.\n",
    "      \n",
    "      max_iter: Int. Maximum number of iterations in the k-medoids step.\n",
    "      \n",
    "      n_attempts: Int. Number of attempts for k-medoids. After n_attempts attempts, the partition with the lowest within-\n",
    "            cluster-sum-of-squares is selected.\n",
    "      \n",
    "      k_medoids_type: String. If \"normal,\" standard k-medoids is employed. If \"minkowski\", Minkowski weighted k-medoids is\n",
    "            used. (This is unrelated to the Minkowski distance metric).\n",
    "      \n",
    "      weight_adjustment: Float. Only relevant if using_weights. The extent to which weights can be adjusted during the\n",
    "            k-medoids.\n",
    "      \n",
    "      eig_extent: Nonnegative int. Only relevant if using_weights. Partitions gain more weight when they have more mutual\n",
    "            information with other partitions. As eig_extent goes to infinity, this becomes the only criterion.\n",
    "                  \n",
    "      using_pca: Boolean. If True, PCA is applied to the data first.\n",
    "    \n",
    "    Methods:\n",
    "    \n",
    "    fit_predict:\n",
    "        Input: 2D numpy array, each row a data point, each column a feature\n",
    "        Output: 1D numpy array with cluster assignments for each data point\n",
    "      \n",
    "    \"\"\"\n",
    "    def __init__(self, k,\n",
    "                n_trees=150,\n",
    "                n_features_to_predict=0.5,\n",
    "                max_depth=5,\n",
    "                using_weights=False,\n",
    "                weight_extent=1,\n",
    "                max_iter=60,\n",
    "                n_attempts=10,\n",
    "                kmedoids_type='normal',\n",
    "                weight_adjustment=0,\n",
    "                eig_extent=0,\n",
    "                using_pca=False,\n",
    "                n_jobs=1):\n",
    "        rft = RFTransform(n_trees,\n",
    "                        n_features_to_predict=n_features_to_predict,\n",
    "                        max_depth=max_depth,\n",
    "                        outputting_weights=using_weights,\n",
    "                        using_pca=using_pca,\n",
    "                        weight_extent=weight_extent,\n",
    "                        n_jobs=n_jobs)\n",
    "        ew = EigenvectorWeighting(extent=eig_extent)\n",
    "        if kmedoids_type == 'normal':\n",
    "            jk = JKMedoids(k,\n",
    "                            max_iter=max_iter,\n",
    "                            n_attempts=n_attempts,\n",
    "                            accepting_weights=using_weights,\n",
    "                            weight_adjustment=weight_adjustment,\n",
    "                            n_jobs=n_jobs)\n",
    "        else:\n",
    "            jk = SquishyJKMedoids(k,\n",
    "                            max_iter=max_iter,\n",
    "                            n_attempts=n_attempts,\n",
    "                            accepting_weights=using_weights,\n",
    "                            weight_adjustment=weight_adjustment,\n",
    "                            n_jobs=n_jobs)\n",
    "        if eig_extent == 0 or not using_weights:\n",
    "            Pipeline.__init__(self,[('rft', rft), ('jkmeans', jk)])\n",
    "        else:\n",
    "            Pipeline.__init__(self,[('rft', rft), ('ew', ew), ('jkmeans', jk)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_boston().data\n",
    "rfc = RFCluster(k=5)\n",
    "clusters = rfc.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
