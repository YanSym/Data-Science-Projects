{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 3. Import libraries and modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from itertools import groupby\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "np.random.seed(123)  # for reproducibility\n",
    " \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from keras.applications import VGG16\n",
    "\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful methods for plotting results\n",
    "class HelperClass(object):\n",
    "        \n",
    "    # helper method\n",
    "    @staticmethod\n",
    "    def stars():\n",
    "        print (\"***********************\")\n",
    "    \n",
    "    # print metrics as dataframe\n",
    "    @staticmethod\n",
    "    def print_dataframe(values):\n",
    "    \n",
    "        metrics_print = ['True Positive', 'True Negative', 'False Negative', 'False Positive',\n",
    "    'Accuracy', 'Precision', 'Recall', 'Specificity', 'F1 Score', 'Roc Auc Score']\n",
    "\n",
    "        df_metrics = pd.DataFrame(\n",
    "            {'Metrics': metrics_print,\n",
    "             'Values': values\n",
    "            })\n",
    "\n",
    "        print (df_metrics.to_string(header=False, index=False))\n",
    "        HelperClass.stars()\n",
    "        \n",
    "                \n",
    "        \n",
    "    # Disclaimer: metodo obtido em:\n",
    "    # http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    # Metodo para plotar a matrix de confusao\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(cm, classes,\n",
    "                              normalize=False,\n",
    "                              title='Confusion matrix',\n",
    "                              cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        This function prints and plots the confusion matrix.\n",
    "        Normalization can be applied by setting `normalize=True`.\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(\"Normalized confusion matrix:\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization:')\n",
    "\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "\n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ROC curve\n",
    "    @staticmethod\n",
    "    def plot_roc_curve(y_true, y_scores):\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "        HelperClass.stars()\n",
    "        print (\"Roc Curve:\")\n",
    "        HelperClass.stars()\n",
    "\n",
    "        plt.plot(fpr, tpr, label = 'ROC Curve', linewidth = 2)\n",
    "        plt.plot([0,1],[0,1], 'k--', linewidth = 2)\n",
    "        plt.title('ROC Curve')\n",
    "        plt.xlim([0.0, 0.001])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # Precision Recall Curve\n",
    "    @staticmethod\n",
    "    def plot_precision_recall_curve(y_true, y_scores):\n",
    "\n",
    "        HelperClass.stars()\n",
    "        print (\"Precision Recall Curve:\")\n",
    "        HelperClass.stars()\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "        plt.step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n",
    "\n",
    "        plt.plot(recall, precision, linewidth=2)\n",
    "        plt.xlim([0.0,1.0])\n",
    "        plt.ylim([0.0,1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision Recall Curve')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # Show classification report\n",
    "    @staticmethod\n",
    "    def show_full_classification_report(y_true, y_pred, y_scores, classes):\n",
    "\n",
    "        HelperClass.stars()\n",
    "        print (\"Metrics Report:\")\n",
    "        HelperClass.stars()\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        true_positive = cm[1,1]\n",
    "        true_negative = cm[0,0]\n",
    "        false_negative = cm[1,0]\n",
    "        false_positive = cm[0,1]\n",
    "\n",
    "        accuracy = ((true_positive + true_negative)/(true_positive + true_negative + false_negative + false_positive))\n",
    "        precision = (true_positive/(true_positive + false_positive))\n",
    "        recall = (true_positive/(true_positive + false_negative))  \n",
    "        sensitivity = (true_positive/(true_positive + false_negative))  \n",
    "        specificity = (true_negative/(true_negative + false_positive))  \n",
    "        f1_score = ((2 * precision * recall)/(precision + recall))\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "        print (\"Classification Report:\")\n",
    "        HelperClass.stars()\n",
    "\n",
    "        values = []\n",
    "\n",
    "        values.append(true_positive)\n",
    "        values.append(true_negative)\n",
    "        values.append(false_negative)\n",
    "        values.append(false_positive)\n",
    "        values.append('{:.4f}'.format(accuracy))\n",
    "        values.append('{:.4f}'.format(precision))\n",
    "        values.append('{:.4f}'.format(recall))\n",
    "        values.append('{:.4f}'.format(specificity))\n",
    "        values.append('{:.4f}'.format(f1_score))\n",
    "        values.append('{:.4f}'.format(roc_auc))\n",
    "\n",
    "        HelperClass.print_dataframe(values)\n",
    "\n",
    "        HelperClass.plot_confusion_matrix(cm, classes)\n",
    "\n",
    "        HelperClass.plot_roc_curve(y_true, y_scores)\n",
    "\n",
    "        HelperClass.plot_precision_recall_curve(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "0\n",
      "(60000, 28, 28, 1)\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"glorot_uniform\", input_shape=(28, 28, 1...)`\n",
      "/home/yan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"glorot_uniform\")`\n",
      "/home/yan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:62: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 124s 2ms/step - loss: 0.2632 - acc: 0.9210\n",
      "  288/10000 [..............................] - ETA: 6s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/home/yan/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 516us/step\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print (X_train.shape)\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# 5. Preprocess input data\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "    print (1)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    print (0)\n",
    "    \n",
    "\n",
    "print (X_train.shape)\n",
    "    \n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "print (y_train[:10])\n",
    " \n",
    "# Preprocess class labels\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "print (y_train)\n",
    "print (y_test)\n",
    " \n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Convolution2D(32, 3, 3, activation='relu', kernel_initializer='glorot_uniform', input_shape=input_shape))\n",
    "model.add(Convolution2D(32, 3, 3, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='bestmodel.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=15, verbose=1)\n",
    " \n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "# Fit model on training data\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32, nb_epoch=1, verbose=1, callbacks=[checkpointer, earlystopper])\n",
    " \n",
    "# Evaluate model on test data\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(X_test)\n",
    "y_pred_bool = np.zeros_like(y_pred)\n",
    "y_pred_bool[np.arange(len(y_pred)), y_pred.argmax(1)] = 1\n",
    "\n",
    "print (roc_auc_score(y_test, y_pred))\n",
    "print (roc_auc_score(y_test, y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score_multiclass(actual_class, pred_class):\n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "    return roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_list = y_test.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = y_pred_bool.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score_multiclass(actual_list, pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print (X_train.shape)\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# 5. Preprocess input data\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "    print (1)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    print (0)\n",
    "    \n",
    "\n",
    "print (X_train.shape)\n",
    "    \n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "print (y_train[:10])\n",
    " \n",
    "# Preprocess class labels\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "print (y_train)\n",
    "print (y_test)\n",
    " \n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=input_shape))\n",
    "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "my_callbacks = [EarlyStopping(monitor='auc_roc', patience=300, verbose=1, mode='max')]\n",
    " \n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.01),\n",
    "              metrics=['accuracy', auc_roc])\n",
    " \n",
    "# Fit model on training data\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32, nb_epoch=2, verbose=1, callbacks=my_callbacks)\n",
    " \n",
    "# Evaluate model on test data\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ZFTurbo/KAGGLE_DISTRACTED_DRIVER/blob/master/run_keras_simple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "from keras.preprocessing import image\n",
    " \n",
    "PATH = os.getcwd()\n",
    " \n",
    "train_path = PATH+'/data/train/'\n",
    "train_batch = os.listdir(train_path)\n",
    "x_train = []\n",
    " \n",
    "# if data are in form of images\n",
    "for sample in train_data:\n",
    "    img_path = train_path+sample\n",
    "    x = image.load_img(img_path)\n",
    "    # preprocessing if required\n",
    "    x_train.append(x)\n",
    "    \n",
    "\n",
    "test_path = PATH+'/data/test/'\n",
    "test_batch = os.listdir(test_path)\n",
    "x_test = []\n",
    " \n",
    "for sample in test_data:\n",
    "    img_path = test_path+sample\n",
    "    x = image.load_img(img_path)\n",
    "    # preprocessing if required\n",
    "    x_test.append(x)\n",
    "\n",
    "# finally converting list into numpy array\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_conv = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './clean-dataset/train'\n",
    "validation_dir = './clean-dataset/validation'\n",
    " \n",
    "nTrain = 600\n",
    "nVal = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pokemon CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/yan/dataset/bulbasaur/00000000.png'\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "print (img.shape)\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "cv2.imshow(\"Input\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mewtwo', 'charmander', 'pikachu', 'bulbasaur', 'squirtle']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "from keras.preprocessing import image\n",
    " \n",
    "\n",
    "train_path = '/home/yan/dataset/'\n",
    "list_folders = os.listdir(train_path)\n",
    "\n",
    "print (list_folders)\n",
    "\n",
    "# initialize the data and labels\n",
    "X_train = []\n",
    "y_train = []\n",
    " \n",
    "# if data are in form of images\n",
    "for folder in list_folders:\n",
    "    full_folder = train_path + folder\n",
    "    #print (full_folder)\n",
    "    for img in os.listdir(full_folder):\n",
    "        img_path = (full_folder + '/' + img)\n",
    "        #print (img_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.resize(image, (900, 900))\n",
    "        image = image.astype('float32')\n",
    "        image = image / 255\n",
    "        image = img_to_array(image)\n",
    "        \n",
    "        X_train.append(image)      \n",
    "        y_train.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(427, 900, 900, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 900, 3)\n"
     ]
    }
   ],
   "source": [
    "print (X_train[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X_train,\n",
    "    y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, (3, 3), padding=\"same\", kernel_initializer='glorot_uniform', input_shape=(900, 900, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3), kernel_initializer='glorot_uniform', padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(Convolution2D(64, (3, 3), kernel_initializer='glorot_uniform', padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(128, (3, 3), kernel_initializer='glorot_uniform', padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(Convolution2D(128, (3, 3), kernel_initializer='glorot_uniform', padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# first (and only) set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# softmax classifier\n",
    "model.add(Dense(5))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "                        height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "                        horizontal_flip=True, vertical_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 341 arrays: [array([[[0.10980392, 0.00392157, 0.15686275],\n        [0.10980392, 0.00392157, 0.15686275],\n        [0.11764706, 0.        , 0.16078432],\n        ...,\n        [0.13333334, 0.        , 0.16078432],\n  ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-8356a4489926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Fit model on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m model.fit(X_train, y_train, \n\u001b[0;32m---> 11\u001b[0;31m           batch_size=32, nb_epoch=1, verbose=1, callbacks=[checkpointer, earlystopper])\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Evaluate model on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 341 arrays: [array([[[0.10980392, 0.00392157, 0.15686275],\n        [0.10980392, 0.00392157, 0.15686275],\n        [0.11764706, 0.        , 0.16078432],\n        ...,\n        [0.13333334, 0.        , 0.16078432],\n  ..."
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='bestmodel.hdf5', verbose=1, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=15, verbose=1)\n",
    " \n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "# Fit model on training data\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32, nb_epoch=1, verbose=1, callbacks=[checkpointer, earlystopper])\n",
    " \n",
    "# Evaluate model on test data\n",
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
